{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Vk3KEzbOiZp",
    "outputId": "5ed1ddb8-5c0e-4840-9686-d32c189e2520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
    "import os\n",
    "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
    "os.environ['DISPLAY']=':1.0'    # tell X clients to use our virtual DISPLAY :1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os0-kuSDS8d5"
   },
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95BRXt7cjS9w",
    "outputId": "1d3dea17-2b1d-4f10-bd93-73c2d1417c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"all\")\n",
    "nlp = en_core_web_sm.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te25x02KUHaY"
   },
   "source": [
    "# NER using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ligMzqKHTH70"
   },
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2fA0-r2MjeWu"
   },
   "outputs": [],
   "source": [
    "RootPath = r'/content/drive/MyDrive/ACU_OCR_TXT'\n",
    "ProcessData , fileNames = [], []\n",
    "for filename in os.listdir(RootPath):\n",
    "    if filename.endswith(\".txt\") :\n",
    "      dataPath = os.path.join(RootPath, filename)\n",
    "      f = open(dataPath, \"r\")\n",
    "      ProcessData.append(f.read())\n",
    "      fileNames.append(filename)\n",
    "    else:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_PharhVEjoBp"
   },
   "outputs": [],
   "source": [
    "inputData = pd.DataFrame({\"FileName\":fileNames, \"Text\":ProcessData}) # Creating Data Frame of all the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Q29o8Tv_ERKE"
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "  \"\"\"\n",
    "  This function modifies the text like won't -> will not\n",
    "  \"\"\"\n",
    "  # specific\n",
    "  phrase = phrase.replace(\"\\n\", \" \")\n",
    "  phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "  phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "  # general\n",
    "  phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "  phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "  return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zilMJ74vE23W"
   },
   "outputs": [],
   "source": [
    "inputData['ContractedData'] = inputData['Text'].apply(decontracted) # Creating the modified texxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmQ6svgOT-1K"
   },
   "source": [
    "## Tokenization using spaCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b9Z8SI58IsSz"
   },
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "def TokenizeText(text):\n",
    "  \"\"\" \n",
    "  Tokenize the text intput into words\n",
    "  \"\"\"\n",
    "\n",
    "  # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "  nlp = English()\n",
    "  #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "  my_doc = nlp(text)\n",
    "  # Create list of word tokens\n",
    "  token_string = ''\n",
    "  for token in my_doc:\n",
    "      token_string += token.text + ' '\n",
    "  #token_string = token_string.replace(\"\\n\", \" \")\n",
    "  return token_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WuDsopV7FN98"
   },
   "outputs": [],
   "source": [
    "inputData['TokenisedData'] = inputData['ContractedData'].apply(TokenizeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "axXtCppEOq7D"
   },
   "outputs": [],
   "source": [
    "def ApplyLemma(text):\n",
    "  \"\"\"\n",
    "  Apply Lemmatization on the text, this is a kind of noarmalization technique on the text data \n",
    "  \"\"\"\n",
    "  nlp = spacy.load('en') # Load english language/ dictionary\n",
    "  doc = nlp(text)\n",
    "  finText = \"\"\n",
    "  for token in doc:\n",
    "      finText += \" \" + token.lemma_\n",
    "  return finText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-aNeUPhAOkBR"
   },
   "outputs": [],
   "source": [
    "inputData['LemmetizedData'] = inputData['TokenisedData'].apply(ApplyLemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "8mB2ww3cJaOe",
    "outputId": "99ba605f-ff94-4ff3-bda0-054a2d43e41f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Text</th>\n",
       "      <th>ContractedData</th>\n",
       "      <th>TokenisedData</th>\n",
       "      <th>LemmetizedData</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACU0000042_ocr.txt</td>\n",
       "      <td>2\\n3\\n5\\n11\\n9 |.\\n21\\n14\\n13\\n12\\nMETRIC 1\\n5...</td>\n",
       "      <td>2 3 5 11 9 |. 21 14 13 12 METRIC 1 5 ABILENE C...</td>\n",
       "      <td>2 3 5 11 9 | . 21 14 13 12 METRIC 1 5 ABILENE ...</td>\n",
       "      <td>2 3 5 11 9 | . 21 14 13 12 METRIC 1 5 abilene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACU0000024_ocr.txt</td>\n",
       "      <td>2\\n3\\n12\\n101\\n6\\n8\\n14\\n13\\n11\\nMETRIC 1\\n2\\n...</td>\n",
       "      <td>2 3 12 101 6 8 14 13 11 METRIC 1 2 ABILENE CHR...</td>\n",
       "      <td>2 3 12 101 6 8 14 13 11 METRIC 1 2 ABILENE CHR...</td>\n",
       "      <td>2 3 12 101 6 8 14 13 11 metric 1 2 abilene CH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACU0000006_ocr.txt</td>\n",
       "      <td>5\\n2\\n3.\\n9\\nABILENE CHRISTIAN COLLEGE\\n6 | 8\\...</td>\n",
       "      <td>5 2 3. 9 ABILENE CHRISTIAN COLLEGE 6 | 8 METRI...</td>\n",
       "      <td>5 2 3 . 9 ABILENE CHRISTIAN COLLEGE 6 | 8 METR...</td>\n",
       "      <td>5 2 3 . 9 abilene CHRISTIAN college 6 | 8 met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACU0000023_ocr.txt</td>\n",
       "      <td>2\\n3\\n5\\n14\\n12\\n11\\n13\\nMETRIC 1\\n3.\\n2\\nABIL...</td>\n",
       "      <td>2 3 5 14 12 11 13 METRIC 1 3. 2 ABILENE CHRIST...</td>\n",
       "      <td>2 3 5 14 12 11 13 METRIC 1 3 . 2 ABILENE CHRIS...</td>\n",
       "      <td>2 3 5 14 12 11 13 METRIC 1 3 . 2 abilene CHRI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACU0000039_ocr.txt</td>\n",
       "      <td>2\\n3.\\n5\\n6\\n13\\n6 |\\n8\\n31\\n21\\n14\\nMETRIC 1\\...</td>\n",
       "      <td>2 3. 5 6 13 6 | 8 31 21 14 METRIC 1 12 ABILENE...</td>\n",
       "      <td>2 3 . 5 6 13 6 | 8 31 21 14 METRIC 1 12 ABILEN...</td>\n",
       "      <td>2 3 . 5 6 13 6 | 8 31 21 14 METRIC 1 12 ABILE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ACU0000030_ocr.txt</td>\n",
       "      <td>2\\n3.\\n13\\n11\\n6\\n8\\n9.\\n14\\nMETRIC 1\\n12\\n2\\n...</td>\n",
       "      <td>2 3. 13 11 6 8 9. 14 METRIC 1 12 2 ABILENE CHR...</td>\n",
       "      <td>2 3 . 13 11 6 8 9 . 14 METRIC 1 12 2 ABILENE C...</td>\n",
       "      <td>2 3 . 13 11 6 8 9 . 14 metric 1 12 2 abilene ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ACU0000079_ocr.txt</td>\n",
       "      <td>ES\\n2\\n3\\n6.\\nABILENE CHRISTIAN COLLEGE\\n14\\nH...</td>\n",
       "      <td>ES 2 3 6. ABILENE CHRISTIAN COLLEGE 14 HERBARI...</td>\n",
       "      <td>ES 2 3 6 . ABILENE CHRISTIAN COLLEGE 14 HERBAR...</td>\n",
       "      <td>ES 2 3 6 . abilene CHRISTIAN COLLEGE 14 herba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ACU0000019_ocr.txt</td>\n",
       "      <td>2\\n3.\\n6.\\n13\\n6 |\\n8\\n21\\n14\\nMETRIC 1\\n12\\n5...</td>\n",
       "      <td>2 3. 6. 13 6 | 8 21 14 METRIC 1 12 5 ABILENE C...</td>\n",
       "      <td>2 3 . 6 . 13 6 | 8 21 14 METRIC 1 12 5 ABILENE...</td>\n",
       "      <td>2 3 . 6 . 13 6 | 8 21 14 METRIC 1 12 5 abilen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ACU0000007_ocr.txt</td>\n",
       "      <td>5\\n6\\n2\\n3\\n9 |\\nMETRIC 1\\n101\\n6 |\\n8\\n2\\n11\\...</td>\n",
       "      <td>5 6 2 3 9 | METRIC 1 101 6 | 8 2 11 14 12 ABIL...</td>\n",
       "      <td>5 6 2 3 9 | METRIC 1 101 6 | 8 2 11 14 12 ABIL...</td>\n",
       "      <td>5 6 2 3 9 | METRIC 1 101 6 | 8 2 11 14 12 ABI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACU0000086_ocr.txt</td>\n",
       "      <td>2\\n3.\\n11\\n31\\n14\\n13\\n12\\nMETRIC 1\\n8\\n2\\nIMA...</td>\n",
       "      <td>2 3. 11 31 14 13 12 METRIC 1 8 2 IMAGED 20 NOV...</td>\n",
       "      <td>2 3 . 11 31 14 13 12 METRIC 1 8 2 IMAGED 20 NO...</td>\n",
       "      <td>2 3 . 11 31 14 13 12 METRIC 1 8 2 imaged 20 N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FileName  ...                                     LemmetizedData\n",
       "0  ACU0000042_ocr.txt  ...   2 3 5 11 9 | . 21 14 13 12 METRIC 1 5 abilene...\n",
       "1  ACU0000024_ocr.txt  ...   2 3 12 101 6 8 14 13 11 metric 1 2 abilene CH...\n",
       "2  ACU0000006_ocr.txt  ...   5 2 3 . 9 abilene CHRISTIAN college 6 | 8 met...\n",
       "3  ACU0000023_ocr.txt  ...   2 3 5 14 12 11 13 METRIC 1 3 . 2 abilene CHRI...\n",
       "4  ACU0000039_ocr.txt  ...   2 3 . 5 6 13 6 | 8 31 21 14 METRIC 1 12 ABILE...\n",
       "5  ACU0000030_ocr.txt  ...   2 3 . 13 11 6 8 9 . 14 metric 1 12 2 abilene ...\n",
       "6  ACU0000079_ocr.txt  ...   ES 2 3 6 . abilene CHRISTIAN COLLEGE 14 herba...\n",
       "7  ACU0000019_ocr.txt  ...   2 3 . 6 . 13 6 | 8 21 14 METRIC 1 12 5 abilen...\n",
       "8  ACU0000007_ocr.txt  ...   5 6 2 3 9 | METRIC 1 101 6 | 8 2 11 14 12 ABI...\n",
       "9  ACU0000086_ocr.txt  ...   2 3 . 11 31 14 13 12 METRIC 1 8 2 imaged 20 N...\n",
       "\n",
       "[10 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1FDzGmRs7dij"
   },
   "outputs": [],
   "source": [
    "filter_sent = lambda x : ' '.join( re.findall('\\w+',x )) # filter only alpha numeric values from the text\n",
    "inputData['FinalData'] = inputData['LemmetizedData'].apply(filter_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "9HXm-glK8xJL",
    "outputId": "226f3ade-5e4a-4ea2-93a3-524997042784"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Text</th>\n",
       "      <th>ContractedData</th>\n",
       "      <th>TokenisedData</th>\n",
       "      <th>LemmetizedData</th>\n",
       "      <th>test</th>\n",
       "      <th>FinalData</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACU0000042_ocr.txt</td>\n",
       "      <td>2\\n3\\n5\\n11\\n9 |.\\n21\\n14\\n13\\n12\\nMETRIC 1\\n5...</td>\n",
       "      <td>2 3 5 11 9 |. 21 14 13 12 METRIC 1 5 ABILENE C...</td>\n",
       "      <td>2 3 5 11 9 | . 21 14 13 12 METRIC 1 5 ABILENE ...</td>\n",
       "      <td>2 3 5 11 9 | . 21 14 13 12 METRIC 1 5 abilene...</td>\n",
       "      <td>2 3 5 11 9 21 14 13 12 METRIC 1 5 abilene CHRI...</td>\n",
       "      <td>2 3 5 11 9 21 14 13 12 METRIC 1 5 abilene CHRI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACU0000024_ocr.txt</td>\n",
       "      <td>2\\n3\\n12\\n101\\n6\\n8\\n14\\n13\\n11\\nMETRIC 1\\n2\\n...</td>\n",
       "      <td>2 3 12 101 6 8 14 13 11 METRIC 1 2 ABILENE CHR...</td>\n",
       "      <td>2 3 12 101 6 8 14 13 11 METRIC 1 2 ABILENE CHR...</td>\n",
       "      <td>2 3 12 101 6 8 14 13 11 metric 1 2 abilene CH...</td>\n",
       "      <td>2 3 12 101 6 8 14 13 11 metric 1 2 abilene CHR...</td>\n",
       "      <td>2 3 12 101 6 8 14 13 11 metric 1 2 abilene CHR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACU0000006_ocr.txt</td>\n",
       "      <td>5\\n2\\n3.\\n9\\nABILENE CHRISTIAN COLLEGE\\n6 | 8\\...</td>\n",
       "      <td>5 2 3. 9 ABILENE CHRISTIAN COLLEGE 6 | 8 METRI...</td>\n",
       "      <td>5 2 3 . 9 ABILENE CHRISTIAN COLLEGE 6 | 8 METR...</td>\n",
       "      <td>5 2 3 . 9 abilene CHRISTIAN college 6 | 8 met...</td>\n",
       "      <td>5 2 3 9 abilene CHRISTIAN college 6 8 metric 1...</td>\n",
       "      <td>5 2 3 9 abilene CHRISTIAN college 6 8 metric 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACU0000023_ocr.txt</td>\n",
       "      <td>2\\n3\\n5\\n14\\n12\\n11\\n13\\nMETRIC 1\\n3.\\n2\\nABIL...</td>\n",
       "      <td>2 3 5 14 12 11 13 METRIC 1 3. 2 ABILENE CHRIST...</td>\n",
       "      <td>2 3 5 14 12 11 13 METRIC 1 3 . 2 ABILENE CHRIS...</td>\n",
       "      <td>2 3 5 14 12 11 13 METRIC 1 3 . 2 abilene CHRI...</td>\n",
       "      <td>2 3 5 14 12 11 13 METRIC 1 3 2 abilene CHRISTI...</td>\n",
       "      <td>2 3 5 14 12 11 13 METRIC 1 3 2 abilene CHRISTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACU0000039_ocr.txt</td>\n",
       "      <td>2\\n3.\\n5\\n6\\n13\\n6 |\\n8\\n31\\n21\\n14\\nMETRIC 1\\...</td>\n",
       "      <td>2 3. 5 6 13 6 | 8 31 21 14 METRIC 1 12 ABILENE...</td>\n",
       "      <td>2 3 . 5 6 13 6 | 8 31 21 14 METRIC 1 12 ABILEN...</td>\n",
       "      <td>2 3 . 5 6 13 6 | 8 31 21 14 METRIC 1 12 ABILE...</td>\n",
       "      <td>2 3 5 6 13 6 8 31 21 14 METRIC 1 12 ABILENE CH...</td>\n",
       "      <td>2 3 5 6 13 6 8 31 21 14 METRIC 1 12 ABILENE CH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ACU0000030_ocr.txt</td>\n",
       "      <td>2\\n3.\\n13\\n11\\n6\\n8\\n9.\\n14\\nMETRIC 1\\n12\\n2\\n...</td>\n",
       "      <td>2 3. 13 11 6 8 9. 14 METRIC 1 12 2 ABILENE CHR...</td>\n",
       "      <td>2 3 . 13 11 6 8 9 . 14 METRIC 1 12 2 ABILENE C...</td>\n",
       "      <td>2 3 . 13 11 6 8 9 . 14 metric 1 12 2 abilene ...</td>\n",
       "      <td>2 3 13 11 6 8 9 14 metric 1 12 2 abilene CHRIS...</td>\n",
       "      <td>2 3 13 11 6 8 9 14 metric 1 12 2 abilene CHRIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ACU0000079_ocr.txt</td>\n",
       "      <td>ES\\n2\\n3\\n6.\\nABILENE CHRISTIAN COLLEGE\\n14\\nH...</td>\n",
       "      <td>ES 2 3 6. ABILENE CHRISTIAN COLLEGE 14 HERBARI...</td>\n",
       "      <td>ES 2 3 6 . ABILENE CHRISTIAN COLLEGE 14 HERBAR...</td>\n",
       "      <td>ES 2 3 6 . abilene CHRISTIAN COLLEGE 14 herba...</td>\n",
       "      <td>ES 2 3 6 abilene CHRISTIAN COLLEGE 14 herbariu...</td>\n",
       "      <td>ES 2 3 6 abilene CHRISTIAN COLLEGE 14 herbariu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ACU0000019_ocr.txt</td>\n",
       "      <td>2\\n3.\\n6.\\n13\\n6 |\\n8\\n21\\n14\\nMETRIC 1\\n12\\n5...</td>\n",
       "      <td>2 3. 6. 13 6 | 8 21 14 METRIC 1 12 5 ABILENE C...</td>\n",
       "      <td>2 3 . 6 . 13 6 | 8 21 14 METRIC 1 12 5 ABILENE...</td>\n",
       "      <td>2 3 . 6 . 13 6 | 8 21 14 METRIC 1 12 5 abilen...</td>\n",
       "      <td>2 3 6 13 6 8 21 14 METRIC 1 12 5 abilene CHRIS...</td>\n",
       "      <td>2 3 6 13 6 8 21 14 METRIC 1 12 5 abilene CHRIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ACU0000007_ocr.txt</td>\n",
       "      <td>5\\n6\\n2\\n3\\n9 |\\nMETRIC 1\\n101\\n6 |\\n8\\n2\\n11\\...</td>\n",
       "      <td>5 6 2 3 9 | METRIC 1 101 6 | 8 2 11 14 12 ABIL...</td>\n",
       "      <td>5 6 2 3 9 | METRIC 1 101 6 | 8 2 11 14 12 ABIL...</td>\n",
       "      <td>5 6 2 3 9 | METRIC 1 101 6 | 8 2 11 14 12 ABI...</td>\n",
       "      <td>5 6 2 3 9 METRIC 1 101 6 8 2 11 14 12 ABILENE ...</td>\n",
       "      <td>5 6 2 3 9 METRIC 1 101 6 8 2 11 14 12 ABILENE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACU0000086_ocr.txt</td>\n",
       "      <td>2\\n3.\\n11\\n31\\n14\\n13\\n12\\nMETRIC 1\\n8\\n2\\nIMA...</td>\n",
       "      <td>2 3. 11 31 14 13 12 METRIC 1 8 2 IMAGED 20 NOV...</td>\n",
       "      <td>2 3 . 11 31 14 13 12 METRIC 1 8 2 IMAGED 20 NO...</td>\n",
       "      <td>2 3 . 11 31 14 13 12 METRIC 1 8 2 imaged 20 N...</td>\n",
       "      <td>2 3 11 31 14 13 12 METRIC 1 8 2 imaged 20 NOV ...</td>\n",
       "      <td>2 3 11 31 14 13 12 METRIC 1 8 2 imaged 20 NOV ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FileName  ...                                          FinalData\n",
       "0  ACU0000042_ocr.txt  ...  2 3 5 11 9 21 14 13 12 METRIC 1 5 abilene CHRI...\n",
       "1  ACU0000024_ocr.txt  ...  2 3 12 101 6 8 14 13 11 metric 1 2 abilene CHR...\n",
       "2  ACU0000006_ocr.txt  ...  5 2 3 9 abilene CHRISTIAN college 6 8 metric 1...\n",
       "3  ACU0000023_ocr.txt  ...  2 3 5 14 12 11 13 METRIC 1 3 2 abilene CHRISTI...\n",
       "4  ACU0000039_ocr.txt  ...  2 3 5 6 13 6 8 31 21 14 METRIC 1 12 ABILENE CH...\n",
       "5  ACU0000030_ocr.txt  ...  2 3 13 11 6 8 9 14 metric 1 12 2 abilene CHRIS...\n",
       "6  ACU0000079_ocr.txt  ...  ES 2 3 6 abilene CHRISTIAN COLLEGE 14 herbariu...\n",
       "7  ACU0000019_ocr.txt  ...  2 3 6 13 6 8 21 14 METRIC 1 12 5 abilene CHRIS...\n",
       "8  ACU0000007_ocr.txt  ...  5 6 2 3 9 METRIC 1 101 6 8 2 11 14 12 ABILENE ...\n",
       "9  ACU0000086_ocr.txt  ...  2 3 11 31 14 13 12 METRIC 1 8 2 imaged 20 NOV ...\n",
       "\n",
       "[10 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "H1VGdS27jTm4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def getNER(text):\n",
    "  \"\"\"\n",
    "  Creating the Named Entity Relation for the text given as input using Spacy\n",
    "  \"\"\"\n",
    "  nlp = en_core_web_sm.load()\n",
    "  sentence_nlp = nlp(text)\n",
    "  # print named entities in article\n",
    "  NER_List = [(word, word.ent_type_) for word in sentence_nlp if word.ent_type_]\n",
    "  # visualize named entities\n",
    "  #displacy.render(sentence_nlp, style='ent', jupyter=True)\n",
    "  return NER_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "aNtPqwprNvqS"
   },
   "outputs": [],
   "source": [
    "inputData['NER_Data'] = inputData['FinalData'].apply(getNER) # NER for each file is created in the column NER Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "znOJiQpo9cKT"
   },
   "outputs": [],
   "source": [
    "inputData.set_index(\"FileName\", inplace=True) #Setting index as the name of the files, which helps in easy acccess of the data for a particular file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "idlYA0b05B2A",
    "outputId": "f2d1577e-b20d-46c0-c990-274c33c82906"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    5\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " 6 12 11 \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " 8 9 21 14 METRIC 1 13 abilene \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CHRISTIAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " college \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HERBARIUM 003146\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " no \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    WM F MAHLER Dee 1920\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " b0 a abilene \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CHRISTIAN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " college \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HERBARIUM TAYLOR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " COUNTY TEXAS 93 DA TE \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    November 17 1965\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " HABITAT Limestone Bluff ASP comment many plant grow beneath a waterfall \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    IMAGED CO MMO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " n \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MA ME\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " Venus Hair Fern 20 NOV \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2020\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Adiantum Capillus Veneris\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " SCIENTIFIC NAME COLLECTOR Robert F Patton \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    FAMILY NUMBER\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " R acecrto ora \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Abilene Christian University ACUO000001\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " 24colorcard \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    TM\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " cameratrax com</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NER for a given sentence, displacy the NER text\n",
    "nlp = en_core_web_sm.load()\n",
    "# Index the data based on the text file name, then see the NER for the contents of the file by running the below code\n",
    "sentenceNER = nlp(inputData.loc['ACU0000001_ocr.txt', 'test'])\n",
    "displacy.render(sentenceNER, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnY3nWrBWN7s"
   },
   "source": [
    "## NER for the complete data(100 text files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "cAaKKPj3jlyW"
   },
   "outputs": [],
   "source": [
    "named_entities = []\n",
    "for sentence in inputData['test']:\n",
    "  sentence = sentence.replace(\"\\n\", ' ' )\n",
    "  temp_entity_name = ''\n",
    "  temp_named_entity = None\n",
    "  sentence = nlp(sentence)\n",
    "  for word in sentence:\n",
    "      term = word.text \n",
    "      tag = word.ent_type_\n",
    "      if tag:\n",
    "          temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "          temp_named_entity = (temp_entity_name, tag)\n",
    "      else:\n",
    "          if temp_named_entity:\n",
    "              named_entities.append(temp_named_entity)\n",
    "              temp_entity_name = ''\n",
    "              temp_named_entity = None\n",
    "# Overall NER for all the text files\n",
    "entity_frame = pd.DataFrame(named_entities, \n",
    "                        columns=['Entity Name', 'Entity Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fL3bDKIswnWk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Uk718d1A1J8Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "mnSWGEXk1xwQ",
    "outputId": "4ecbd2bc-528c-49f1-eafa-9d705655a49a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Entity Name</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>TM</td>\n",
       "      <td>2 3</td>\n",
       "      <td>Abilene Christian University</td>\n",
       "      <td>CHRISTIAN UNIVERSITY</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>CHRISTIAN COLLEGE</td>\n",
       "      <td>14</td>\n",
       "      <td>CHRISTIAN</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2 3 5</td>\n",
       "      <td>Habitat</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>HABITAT</td>\n",
       "      <td>14 metric</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Barbara Bowles</td>\n",
       "      <td>ABILENE CHRISTIAN</td>\n",
       "      <td>Christian University</td>\n",
       "      <td>Altitude Habitat</td>\n",
       "      <td>8</td>\n",
       "      <td>Soil</td>\n",
       "      <td>ABILENE CHRISTIAN COLLEGE</td>\n",
       "      <td>Pinaceae</td>\n",
       "      <td>Abilene Christian</td>\n",
       "      <td>HERBARIUM</td>\n",
       "      <td>14 6</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>ACC</td>\n",
       "      <td>24colorcard tm</td>\n",
       "      <td>CHRISTIAN COLLEGE HERBARIUM</td>\n",
       "      <td>Herbarium</td>\n",
       "      <td>IMAGED</td>\n",
       "      <td>12 6</td>\n",
       "      <td>101</td>\n",
       "      <td>canadian</td>\n",
       "      <td>1 12</td>\n",
       "      <td>STATE</td>\n",
       "      <td>15</td>\n",
       "      <td>Jason Peters Abilene Christian University</td>\n",
       "      <td>Robert Patton</td>\n",
       "      <td>3 14 metric</td>\n",
       "      <td>83</td>\n",
       "      <td>MM CM</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entity Type</th>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>DATE</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>GPE</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NORP</td>\n",
       "      <td>NORP</td>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>ORG</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NORP</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>CARDINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency</th>\n",
       "      <td>78</td>\n",
       "      <td>49</td>\n",
       "      <td>46</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1    2   ...        47      48        49\n",
       "Entity Name        20         2   TM  ...        83   MM CM        10\n",
       "Entity Type  CARDINAL  CARDINAL  ORG  ...  CARDINAL  PERSON  CARDINAL\n",
       "Frequency          78        49   46  ...         2       2         2\n",
       "\n",
       "[3 rows x 50 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top named entities\n",
    "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
    "                           .size()\n",
    "                           .sort_values(ascending=False)\n",
    "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "top_entities.T.iloc[:,:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "swAL-z3G-tuw",
    "outputId": "d36c78ab-502e-4230-c857-a4a9253f3b46"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Entity Type</th>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>DATE</td>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>NORP</td>\n",
       "      <td>GPE</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>TIME</td>\n",
       "      <td>FAC</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>LOC</td>\n",
       "      <td>PERCENT</td>\n",
       "      <td>LAW</td>\n",
       "      <td>WORK_OF_ART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency</th>\n",
       "      <td>332</td>\n",
       "      <td>314</td>\n",
       "      <td>139</td>\n",
       "      <td>75</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0    1       2     3   ...   11       12   13           14\n",
       "Entity Type  CARDINAL  ORG  PERSON  DATE  ...  LOC  PERCENT  LAW  WORK_OF_ART\n",
       "Frequency         332  314     139    75  ...    4        2    2            1\n",
       "\n",
       "[2 rows x 15 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top(15) named entity types\n",
    "top_entities = (entity_frame.groupby(by=['Entity Type'])\n",
    "                           .size()\n",
    "                           .sort_values(ascending=False)\n",
    "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "top_entities.T.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuQLvopeaG-Q"
   },
   "source": [
    "# NER using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "jxzNI3djBT65"
   },
   "outputs": [],
   "source": [
    "NLTK_Data = pd.DataFrame({\"FileName\":fileNames, \"Text\":ProcessData}) # Data frame creation has data for all the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "B-LOwfhGBd6J"
   },
   "outputs": [],
   "source": [
    "NLTK_Data['ContractedData'] = NLTK_Data['Text'].apply(decontracted) # Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ysvfPh2dL-gs"
   },
   "outputs": [],
   "source": [
    "def lemmatize(textList):\n",
    "  \"\"\"\n",
    "  Data Normalisation technique, lemmatization for text data using NLTK\n",
    "  \"\"\"\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  return [lemmatizer.lemmatize(word) for word in textList] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ifaTzjby-ps9"
   },
   "outputs": [],
   "source": [
    "# TOkenization using NLTK\n",
    "NLTK_Data['ToknData'] = NLTK_Data['ContractedData'].apply(lambda x : nltk.word_tokenize(x))\n",
    "# Stemming using NLTK\n",
    "NLTK_Data['Lemma_Data'] = NLTK_Data['ToknData'].apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "N33GhnU1Gl5R"
   },
   "outputs": [],
   "source": [
    "def GenNER_NLTK(sentence):\n",
    "  \"\"\"\n",
    "  Create NER for a given text file using NLTK\n",
    "  \"\"\"\n",
    "  word_tree = nltk.ne_chunk(nltk.tag.pos_tag(sentence)) # Creating NER for the text\n",
    "  chunked = [] \n",
    "  # Extracting the leaves from the NER trees\n",
    "  for t in word_tree.subtrees():\n",
    "        chunked.append(t)\n",
    "  named_entities = chunked[1:]      \n",
    "  return named_entities\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "zBMJrUkKHdd2"
   },
   "outputs": [],
   "source": [
    "NLTK_Data['NER_data'] = NLTK_Data['Lemma_Data'].apply(GenNER_NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "muU8sxEo_RZ9",
    "outputId": "8c344378-8c40-46df-9596-376a66928e45"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Text</th>\n",
       "      <th>ContractedData</th>\n",
       "      <th>ToknData</th>\n",
       "      <th>Lemma_Data</th>\n",
       "      <th>NER_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACU0000042_ocr.txt</td>\n",
       "      <td>2\\n3\\n5\\n11\\n9 |.\\n21\\n14\\n13\\n12\\nMETRIC 1\\n5...</td>\n",
       "      <td>2 3 5 11 9 |. 21 14 13 12 METRIC 1 5 ABILENE C...</td>\n",
       "      <td>[2, 3, 5, 11, 9, |, ., 21, 14, 13, 12, METRIC,...</td>\n",
       "      <td>[2, 3, 5, 11, 9, |, ., 21, 14, 13, 12, METRIC,...</td>\n",
       "      <td>[[(METRIC, NNP)], [(ABILENE, NNP)], [(CHRISTIA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACU0000024_ocr.txt</td>\n",
       "      <td>2\\n3\\n12\\n101\\n6\\n8\\n14\\n13\\n11\\nMETRIC 1\\n2\\n...</td>\n",
       "      <td>2 3 12 101 6 8 14 13 11 METRIC 1 2 ABILENE CHR...</td>\n",
       "      <td>[2, 3, 12, 101, 6, 8, 14, 13, 11, METRIC, 1, 2...</td>\n",
       "      <td>[2, 3, 12, 101, 6, 8, 14, 13, 11, METRIC, 1, 2...</td>\n",
       "      <td>[[(METRIC, NNP)], [(ABILENE, NNP)], [(Common, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACU0000006_ocr.txt</td>\n",
       "      <td>5\\n2\\n3.\\n9\\nABILENE CHRISTIAN COLLEGE\\n6 | 8\\...</td>\n",
       "      <td>5 2 3. 9 ABILENE CHRISTIAN COLLEGE 6 | 8 METRI...</td>\n",
       "      <td>[5, 2, 3, ., 9, ABILENE, CHRISTIAN, COLLEGE, 6...</td>\n",
       "      <td>[5, 2, 3, ., 9, ABILENE, CHRISTIAN, COLLEGE, 6...</td>\n",
       "      <td>[[(ABILENE, NNP)], [(METRIC, NNP)], [(HERBARIU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACU0000023_ocr.txt</td>\n",
       "      <td>2\\n3\\n5\\n14\\n12\\n11\\n13\\nMETRIC 1\\n3.\\n2\\nABIL...</td>\n",
       "      <td>2 3 5 14 12 11 13 METRIC 1 3. 2 ABILENE CHRIST...</td>\n",
       "      <td>[2, 3, 5, 14, 12, 11, 13, METRIC, 1, 3, ., 2, ...</td>\n",
       "      <td>[2, 3, 5, 14, 12, 11, 13, METRIC, 1, 3, ., 2, ...</td>\n",
       "      <td>[[(METRIC, NNP)], [(ABILENE, NNP)], [(CHRISTIA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACU0000039_ocr.txt</td>\n",
       "      <td>2\\n3.\\n5\\n6\\n13\\n6 |\\n8\\n31\\n21\\n14\\nMETRIC 1\\...</td>\n",
       "      <td>2 3. 5 6 13 6 | 8 31 21 14 METRIC 1 12 ABILENE...</td>\n",
       "      <td>[2, 3, ., 5, 6, 13, 6, |, 8, 31, 21, 14, METRI...</td>\n",
       "      <td>[2, 3, ., 5, 6, 13, 6, |, 8, 31, 21, 14, METRI...</td>\n",
       "      <td>[[(METRIC, NNP)], [(ABILENE, NNP)], [(NO, NNP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FileName  ...                                           NER_data\n",
       "0  ACU0000042_ocr.txt  ...  [[(METRIC, NNP)], [(ABILENE, NNP)], [(CHRISTIA...\n",
       "1  ACU0000024_ocr.txt  ...  [[(METRIC, NNP)], [(ABILENE, NNP)], [(Common, ...\n",
       "2  ACU0000006_ocr.txt  ...  [[(ABILENE, NNP)], [(METRIC, NNP)], [(HERBARIU...\n",
       "3  ACU0000023_ocr.txt  ...  [[(METRIC, NNP)], [(ABILENE, NNP)], [(CHRISTIA...\n",
       "4  ACU0000039_ocr.txt  ...  [[(METRIC, NNP)], [(ABILENE, NNP)], [(NO, NNP)...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLTK_Data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "3iaVL9tjS-tw"
   },
   "outputs": [],
   "source": [
    "NLTK_Data.set_index(\"FileName\",inplace=True) # Setting the File Name column to index which facilitates in easy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "GdG2IqHmS5u9"
   },
   "outputs": [],
   "source": [
    "samplText = NLTK_Data['NER_data']['ACU0000042_ocr.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "FG2-T7D8L46Q"
   },
   "outputs": [],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(samplText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PP2SoYV9H7cX",
    "outputId": "c7ffd7bf-5665-49b6-a3f5-18acbbb4db86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree('S', [Tree('ORGANIZATION', [('METRIC', 'NNP')]), Tree('ORGANIZATION', [('ABILENE', 'NNP')]), Tree('ORGANIZATION', [('CHRISTIAN', 'NNP')]), Tree('ORGANIZATION', [('UNIVERSITY', 'NNP')]), Tree('ORGANIZATION', [('HERBARIUM', 'NNP')]), Tree('ORGANIZATION', [('RIRIRIRIRIA', 'NNP'), ('Herbarium', 'NNP')]), Tree('GPE', [('No', 'NNP')]), Tree('PERSON', [('Family', 'NNP')]), Tree('ORGANIZATION', [('Locality', 'NNP'), ('Colmawcho', 'NNP'), ('Count', 'NNP')]), Tree('ORGANIZATION', [('Date', 'NNP'), ('Altitude', 'NNP'), ('Habitat', 'NNP')]), Tree('PERSON', [('Rhowda', 'NNP')]), Tree('PERSON', [('Abilene', 'NNP'), ('Christian', 'NNP'), ('University', 'NNP')]), Tree('ORGANIZATION', [('RIRIRIRIIRIRIRIRIIRIRIIRIRIRIIRIRIIRIRIII', 'NNP')])])\n"
     ]
    }
   ],
   "source": [
    "pprint(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVr6cJiaMOaB"
   },
   "outputs": [],
   "source": [
    "cs.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60z_WvNSZwG-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NER_Spacy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
